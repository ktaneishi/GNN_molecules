{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, dim, n_fingerprint, hidden_layer, output_layer, update_func, output_func):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        self.W_fingerprint = nn.ModuleList([nn.Linear(dim, dim)] * hidden_layer)\n",
    "        self.W_output = nn.ModuleList([nn.Linear(dim, dim)] * output_layer)\n",
    "        self.W_property = nn.Linear(dim, 2)\n",
    "        self.update_func = update_func\n",
    "        self.output_func = output_func\n",
    "\n",
    "    def pad(self, matrices, pad_value):\n",
    "        '''Pad adjacency matrices for batch processing.'''\n",
    "        sizes = [m.shape[0] for m in matrices]\n",
    "        M = sum(sizes)\n",
    "        pad_matrices = pad_value + np.zeros((M, M))\n",
    "        i = 0\n",
    "        for j, m in enumerate(matrices):\n",
    "            j = sizes[j]\n",
    "            pad_matrices[i:i+j, i:i+j] = m\n",
    "            i += j\n",
    "        return torch.FloatTensor(pad_matrices)\n",
    "\n",
    "    def sum_axis(self, xs, axis):\n",
    "        y = [torch.sum(x, 0) for x in torch.split(xs, axis)]\n",
    "        return torch.stack(y)\n",
    "\n",
    "    def mean_axis(self, xs, axis):\n",
    "        y = [torch.mean(x, 0) for x in torch.split(xs, axis)]\n",
    "        return torch.stack(y)\n",
    "\n",
    "    def update(self, xs, A, M, i):\n",
    "        '''Update the node vectors in a graph considering their neighboring node vectors (i.e., sum or mean),\n",
    "        which are non-linear transformed by neural network.'''\n",
    "        hs = torch.relu(self.W_fingerprint[i](xs))\n",
    "        if self.update_func == 'sum':\n",
    "            return xs + torch.matmul(A, hs)\n",
    "        if self.update_func == 'mean':\n",
    "            return xs + torch.matmul(A, hs) / (M-1)\n",
    "\n",
    "    def forward(self, inputs, device):\n",
    "        Smiles, fingerprints, adjacencies = inputs\n",
    "        axis = [len(f) for f in fingerprints]\n",
    "\n",
    "        M = np.concatenate([np.repeat(len(f), len(f)) for f in fingerprints])\n",
    "        M = torch.unsqueeze(torch.FloatTensor(M), 1)\n",
    "\n",
    "        fingerprints = torch.cat(fingerprints)\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "\n",
    "        adjacencies = self.pad(adjacencies, 0).to(device)\n",
    "\n",
    "        # GNN updates the fingerprint vectors.\n",
    "        for i in range(len(self.W_fingerprint)):\n",
    "            fingerprint_vectors = self.update(fingerprint_vectors, adjacencies, M, i)\n",
    "\n",
    "        if self.output_func == 'sum':\n",
    "            molecular_vectors = self.sum_axis(fingerprint_vectors, axis)\n",
    "        if self.output_func == 'mean':\n",
    "            molecular_vectors = self.mean_axis(fingerprint_vectors, axis)\n",
    "\n",
    "        for j in range(len(self.W_output)):\n",
    "            molecular_vectors = torch.relu(self.W_output[j](molecular_vectors))\n",
    "\n",
    "        molecular_properties = self.W_property(molecular_vectors)\n",
    "\n",
    "        return Smiles, molecular_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, optimizer, batch, device):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    for i in range(0, len(dataset), batch):\n",
    "        data_batch = list(zip(*dataset[i:i+batch]))       \n",
    "        inputs = data_batch[:-1]\n",
    "        correct_properties = torch.cat(data_batch[-1])\n",
    "        Smiles, predicted_properties = model.forward(inputs, device)\n",
    "        loss = F.cross_entropy(predicted_properties, correct_properties)    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.to('cpu').data.numpy()\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, model, batch, device):\n",
    "    model.eval()\n",
    "    SMILES, Ts, Ys, Ss = '', [], [], []\n",
    "\n",
    "    for i in range(0, len(dataset), batch):\n",
    "        data_batch = list(zip(*dataset[i:i+batch]))\n",
    "        inputs = data_batch[:-1]\n",
    "        correct_properties = torch.cat(data_batch[-1])\n",
    "        Smiles, predicted_properties = model.forward(inputs, device)\n",
    "\n",
    "        correct_labels = correct_properties.to('cpu').data.numpy()\n",
    "        ys = F.softmax(predicted_properties, 1).to('cpu').data.numpy()\n",
    "        predicted_labels = [np.argmax(y) for y in ys]\n",
    "        predicted_scores = [x[1] for x in ys]\n",
    "        \n",
    "        SMILES += ' '.join(Smiles) + ' '\n",
    "        Ts.append(correct_labels)\n",
    "        Ys.append(predicted_labels)\n",
    "        Ss.append(predicted_scores)\n",
    "\n",
    "    SMILES = SMILES.strip().split()\n",
    "    T = np.concatenate(Ts)\n",
    "    Y = np.concatenate(Ys)\n",
    "    S = np.concatenate(Ss)\n",
    "\n",
    "    AUC = roc_auc_score(T, S)\n",
    "    precision = precision_score(T, Y)\n",
    "    recall = recall_score(T, Y)\n",
    "\n",
    "    T, Y, S = map(str, T), map(str, Y), map(str, S)\n",
    "    predictions = '\\n'.join(['\\t'.join(p) for p in zip(SMILES, T, Y, S)])\n",
    "\n",
    "    return AUC, precision, recall, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filename, dtype, device):\n",
    "    return [dtype(d).to(device) for d in np.load(filename + '.npy', allow_pickle=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''Hyperparameters.'''\n",
    "    DATASET = 'HIV'\n",
    "    #DATASET = yourdata\n",
    "\n",
    "    #radius = 1\n",
    "    radius = 2\n",
    "    #radius = 3\n",
    "\n",
    "    update_func = 'sum'\n",
    "    #update_func = 'mean'\n",
    "\n",
    "    #output_func = 'sum'\n",
    "    output_func = 'mean'\n",
    "\n",
    "    dim = 25\n",
    "    hidden_layer = 6\n",
    "    output_layer = 3\n",
    "    batch = 32\n",
    "    lr = 1e-3\n",
    "    lr_decay = 0.9\n",
    "    decay_interval = 10\n",
    "    weight_decay = 1e-6\n",
    "    \n",
    "    iteration = 30\n",
    "    \n",
    "    setting = 'default'\n",
    "\n",
    "    # CPU or GPU.\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses GPU...')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU...')\n",
    "\n",
    "    # Load preprocessed data.\n",
    "    dir_input = '../../dataset/classification/%s/input/radius%d/' % (DATASET, radius)\n",
    "    with open(dir_input + 'Smiles.txt') as f:\n",
    "        Smiles = f.read().strip().split()\n",
    "    molecules = load_tensor(dir_input + 'molecules', torch.LongTensor, device)\n",
    "    adjacencies = np.load(dir_input + 'adjacencies' + '.npy', allow_pickle=True)\n",
    "    properties = load_tensor(dir_input + 'properties', torch.LongTensor, device)\n",
    "    with open(dir_input + 'fingerprint_dict.pkl', 'rb') as f:\n",
    "        fingerprint_dict = pickle.load(f)\n",
    "    n_fingerprint = len(fingerprint_dict)\n",
    "\n",
    "    # Create a dataset and split it into train/test.\n",
    "    dataset = list(zip(Smiles, molecules, adjacencies, properties))\n",
    "    np.random.shuffle(dataset)\n",
    "    dataset_train, dataset_test = train_test_split(dataset, train_size=0.8, test_size=0.2)\n",
    "    print(len(dataset), len(dataset_train), len(dataset_test))\n",
    "\n",
    "    # Set a model.\n",
    "    model = GraphNeuralNetwork(dim, n_fingerprint, hidden_layer, output_layer, update_func, output_func)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Output files.\n",
    "    file_AUCs = '../../output/result/AUCs--%s.txt' % setting\n",
    "    file_predictions = '../../output/result/predictions--%s.txt' % setting\n",
    "    file_model = '../../output/model/%s.pth' % setting\n",
    "    columns = ['Epoch', 'Time(sec)', 'Loss_train', 'AUC_test', 'Precision_test', 'Recall_test']\n",
    "    AUCs = '\\t'.join(columns)\n",
    "\n",
    "    # Start training.\n",
    "    print('Training...')\n",
    "    print(AUCs)\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(1, iteration):\n",
    "        if epoch % decay_interval == 0:\n",
    "            optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "        loss_train = train(dataset_train, model, optimizer, batch, device)\n",
    "        AUC_test, precision_test, recall_test, predictions_test = test(dataset_test, model, batch, device)\n",
    "\n",
    "        time = timeit.default_timer() - start\n",
    "\n",
    "        values = [time, loss_train, AUC_test, precision_test, recall_test]\n",
    "        AUCs = str(epoch) + '\\t' + '\\t'.join(map(lambda x: '%.3f' % x, values))\n",
    "        print(AUCs)\n",
    "\n",
    "    with open(file_predictions, 'w') as out:\n",
    "        out.write('\\t'.join(['Smiles', 'Correct', 'Predict', 'Score']) + '\\n')\n",
    "        out.write(predictions_test + '\\n')\n",
    "    torch.save(model.state_dict(), file_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses GPU...\n",
      "38775 31020 7755\n",
      "Training...\n",
      "Epoch\tTime(sec)\tLoss_train\tAUC_test\tPrecision_test\tRecall_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taneishi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t87.231\t138.032\t0.695\t0.000\t0.000\n",
      "2\t196.894\t112.589\t0.747\t0.800\t0.017\n",
      "3\t306.881\t90.401\t0.767\t0.756\t0.130\n",
      "4\t416.464\t75.353\t0.771\t0.787\t0.155\n",
      "5\t526.385\t64.130\t0.774\t0.662\t0.197\n",
      "6\t635.701\t55.514\t0.775\t0.622\t0.192\n",
      "7\t750.605\t48.403\t0.773\t0.512\t0.264\n",
      "8\t868.812\t42.889\t0.772\t0.472\t0.280\n",
      "9\t988.080\t38.130\t0.771\t0.434\t0.301\n",
      "10\t1106.810\t34.559\t0.768\t0.463\t0.289\n",
      "11\t1225.976\t32.697\t0.769\t0.412\t0.314\n",
      "12\t1311.126\t30.445\t0.772\t0.373\t0.339\n",
      "13\t1371.624\t29.736\t0.771\t0.351\t0.301\n",
      "14\t1432.867\t27.174\t0.772\t0.344\t0.326\n",
      "15\t1493.543\t26.141\t0.773\t0.362\t0.314\n",
      "16\t1553.911\t24.163\t0.775\t0.446\t0.259\n",
      "17\t1621.277\t23.137\t0.769\t0.397\t0.289\n",
      "18\t1689.844\t23.509\t0.770\t0.477\t0.218\n",
      "19\t1750.227\t22.008\t0.768\t0.404\t0.230\n",
      "20\t1810.595\t21.805\t0.769\t0.468\t0.272\n",
      "21\t1870.876\t20.075\t0.768\t0.423\t0.243\n",
      "22\t1931.487\t19.065\t0.766\t0.446\t0.259\n",
      "23\t1992.425\t20.041\t0.770\t0.396\t0.272\n",
      "24\t2052.943\t18.371\t0.769\t0.438\t0.264\n",
      "25\t2113.278\t17.863\t0.772\t0.383\t0.268\n",
      "26\t2173.751\t17.383\t0.768\t0.424\t0.280\n",
      "27\t2234.771\t17.504\t0.765\t0.397\t0.305\n",
      "28\t2295.346\t17.711\t0.763\t0.431\t0.247\n",
      "29\t2355.783\t16.719\t0.768\t0.380\t0.251\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
